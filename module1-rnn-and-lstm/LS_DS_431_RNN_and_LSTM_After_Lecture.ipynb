{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 3 Lesson 1*\n",
    "\n",
    "# Recurrent Neural Networks and Long Short Term Memory (LSTM)\n",
    "## _aka_ PREDICTING THE FUTURE!\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/l2JJu8U8SoHhQEnoQ/giphy.gif\" width=480 height=356>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "> \"Yesterday's just a memory - tomorrow is never what it's supposed to be.\" -- Bob Dylan\n",
    "\n",
    "Wish you could save [Time In A Bottle](https://www.youtube.com/watch?v=AnWWj6xOleY)? With statistics you can do the next best thing - understand how data varies over time (or any sequential order), and use the order/time dimension predictively.\n",
    "\n",
    "A sequence is just any enumerated collection - order counts, and repetition is allowed. Python lists are a good elemental example - `[1, 2, 2, -1]` is a valid list, and is different from `[1, 2, -1, 2]`. The data structures we tend to use (e.g. NumPy arrays) are often built on this fundamental structure.\n",
    "\n",
    "A time series is data where you have not just the order but some actual continuous marker for where they lie \"in time\" - this could be a date, a timestamp, [Unix time](https://en.wikipedia.org/wiki/Unix_time), or something else. All time series are also sequences, and for some techniques you may just consider their order and not \"how far apart\" the entries are (if you have particularly consistent data collected at regular intervals it may not matter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44QZgrPUe3-Y"
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "There's plenty more to \"traditional\" time series, but the latest and greatest technique for sequence data is recurrent neural networks. A recurrence relation in math is an equation that uses recursion to define a sequence - a famous example is the Fibonacci numbers:\n",
    "\n",
    "$F_n = F_{n-1} + F_{n-2}$\n",
    "\n",
    "For formal math you also need a base case $F_0=1, F_1=1$, and then the rest builds from there. But for neural networks what we're really talking about are loops:\n",
    "\n",
    "![Recurrent neural network](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The hidden layers have edges (output) going back to their own input - this loop means that for any time `t` the training is at least partly based on the output from time `t-1`. The entire network is being represented on the left, and you can unfold the network explicitly to see how it behaves at any given `t`.\n",
    "\n",
    "Different units can have this \"loop\", but a particularly successful one is the long short-term memory unit (LSTM):\n",
    "\n",
    "![Long short-term memory unit](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Long_Short-Term_Memory.svg/1024px-Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "There's a lot going on here - in a nutshell, the calculus still works out and backpropagation can still be implemented. The advantage (ane namesake) of LSTM is that it can generally put more weight on recent (short-term) events while not completely losing older (long-term) information.\n",
    "\n",
    "After enough iterations, a typical neural network will start calculating prior gradients that are so small they effectively become zero - this is the [vanishing gradient problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem), and is what RNN with LSTM addresses. Pay special attention to the $c_t$ parameters and how they pass through the unit to get an intuition for how this problem is solved.\n",
    "\n",
    "So why are these cool? One particularly compelling application is actually not time series but language modeling - language is inherently ordered data (letters/words go one after another, and the order *matters*). [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a famous and worth reading blog post on this topic.\n",
    "\n",
    "For our purposes, let's use TensorFlow and Keras to train RNNs with natural language. Resources:\n",
    "\n",
    "- https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "- https://keras.io/layers/recurrent/#lstm\n",
    "- http://adventuresinmachinelearning.com/keras-lstm-tutorial/\n",
    "\n",
    "Note that `tensorflow.contrib` [also has an implementation of RNN/LSTM](https://www.tensorflow.org/tutorials/sequences/recurrent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eWrQllf8WEd-"
   },
   "source": [
    "### RNN/LSTM Sentiment Classification with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#Trains an LSTM model on the IMDB sentiment classification task.\n",
    "The dataset is actually too small for LSTM to be of any advantage\n",
    "compared to simpler, much faster methods such as TF-IDF + LogReg.\n",
    "**Notes**\n",
    "- RNNs are tricky. Choice of batch size is important,\n",
    "choice of loss and optimizer is critical, etc.\n",
    "Some configurations won't converge.\n",
    "- LSTM loss decrease patterns during training can be quite different\n",
    "from what you see with CNNs/MLPs/etc.\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words (among top max_features most common words)\n",
    "maxlen = 80\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "321"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[2467])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   15,   256,     4,     2,     7,  3766,     5,   723,    36,\n",
       "          71,    43,   530,   476,    26,   400,   317,    46,     7,\n",
       "           4, 12118,  1029,    13,   104,    88,     4,   381,    15,\n",
       "         297,    98,    32,  2071,    56,    26,   141,     6,   194,\n",
       "        7486,    18,     4,   226,    22,    21,   134,   476,    26,\n",
       "         480,     5,   144,    30,  5535,    18,    51,    36,    28,\n",
       "         224,    92,    25,   104,     4,   226,    65,    16,    38,\n",
       "        1334,    88,    12,    16,   283,     5,    16,  4472,   113,\n",
       "         103,    32,    15,    16,  5345,    19,   178,    32],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 975
    },
    "colab_type": "code",
    "id": "Ti23G0gRe3kr",
    "outputId": "bba9ae40-a286-49ed-d87b-b2946fb60ddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0127 11:41:28.526748 4769819968 deprecation.py:323] From /Users/jonathansokoll/anaconda3/envs/U4-S3-DNN/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 75s 3ms/sample - loss: 0.4608 - accuracy: 0.7830 - val_loss: 0.3750 - val_accuracy: 0.8365\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 70s 3ms/sample - loss: 0.3047 - accuracy: 0.8740 - val_loss: 0.3893 - val_accuracy: 0.8296\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 70s 3ms/sample - loss: 0.2173 - accuracy: 0.9155 - val_loss: 0.4214 - val_accuracy: 0.8329\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 71s 3ms/sample - loss: 0.1521 - accuracy: 0.9428 - val_loss: 0.4941 - val_accuracy: 0.8216\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 77s 3ms/sample - loss: 0.1162 - accuracy: 0.9576 - val_loss: 0.4938 - val_accuracy: 0.8240\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 78s 3ms/sample - loss: 0.0818 - accuracy: 0.9712 - val_loss: 0.6287 - val_accuracy: 0.8179\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 77s 3ms/sample - loss: 0.0590 - accuracy: 0.9794 - val_loss: 0.7587 - val_accuracy: 0.8145\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.0464 - accuracy: 0.9833 - val_loss: 0.7539 - val_accuracy: 0.8117\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 77s 3ms/sample - loss: 0.0382 - accuracy: 0.9878 - val_loss: 0.8128 - val_accuracy: 0.8183\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 76s 3ms/sample - loss: 0.0324 - accuracy: 0.9890 - val_loss: 0.9054 - val_accuracy: 0.8148\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 71s 3ms/sample - loss: 0.0198 - accuracy: 0.9932 - val_loss: 0.9458 - val_accuracy: 0.8124\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 82s 3ms/sample - loss: 0.0166 - accuracy: 0.9949 - val_loss: 1.0020 - val_accuracy: 0.8057\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 82s 3ms/sample - loss: 0.0135 - accuracy: 0.9957 - val_loss: 1.1265 - val_accuracy: 0.8134\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 82s 3ms/sample - loss: 0.0127 - accuracy: 0.9958 - val_loss: 1.0269 - val_accuracy: 0.8121\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 82s 3ms/sample - loss: 0.0104 - accuracy: 0.9969 - val_loss: 1.1666 - val_accuracy: 0.8123\n",
      "25000/25000 [==============================] - 23s 906us/sample - loss: 1.1666 - accuracy: 0.8123\n",
      "Test score: 1.166573968951106\n",
      "Test accuracy: 0.81228\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=15,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pETWPIe362y"
   },
   "source": [
    "### LSTM Text generation with Keras\n",
    "\n",
    "What else can we do with LSTMs? Since we're analyzing the *sequence*, we can do more than classify - we can *generate* text. I'ved pulled some news stories using [newspaper](https://github.com/codelucas/newspaper/).\n",
    "\n",
    "This example is drawn from the Keras [documentation](https://keras.io/examples/lstm_text_generation/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = os.listdir('./articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Data\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in data_files:\n",
    "    if file[-3:] == 'txt':\n",
    "        with open(f'./articles/{file}', 'r') as f:\n",
    "            data.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Data as Chars\n",
    "\n",
    "text = \" \".join(data)\n",
    "\n",
    "chars = list(set(text))\n",
    "\n",
    "char_int = {c:i for i,c in enumerate(chars)}\n",
    "int_char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences: 178374\n"
     ]
    }
   ],
   "source": [
    "# Create the Sequence Data\n",
    "\n",
    "maxlen = 40\n",
    "step = 5\n",
    "\n",
    "encoded = [char_int[c] for c in text]\n",
    "\n",
    "sequences = [] # Each element is 40 characters long\n",
    "next_chars = [] # One element for each sequence\n",
    "\n",
    "for i in range(0, len(encoded) - maxlen, step):\n",
    "    sequences.append(encoded[i : i + maxlen])\n",
    "    next_chars.append(encoded[i + maxlen])\n",
    "    \n",
    "print('sequences:', len(sequences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify x & y\n",
    "\n",
    "x = np.zeros((len(sequences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sequences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sequence in enumerate(sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        x[i,t,char] = 1\n",
    "        \n",
    "    y[i, next_chars[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 40, 121)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178374, 121)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: a single LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    \n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    \n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    generated = ''\n",
    "    \n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    \n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_int[char]] = 1\n",
    "            \n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature=1.0)\n",
    "        next_char = int_char[next_index]\n",
    "        \n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def on_epoch_end(epoch, _):\n",
    "#     # Function invoked at end of each epoch. Prints generated text.\n",
    "#     print()\n",
    "#     print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "#     start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "#     for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "#         print('----- diversity:', diversity)\n",
    "\n",
    "#         generated = ''\n",
    "#         sentence = text[start_index: start_index + maxlen]\n",
    "#         generated += sentence\n",
    "#         print('----- Generating with seed: \"' + sentence + '\"')\n",
    "#         sys.stdout.write(generated)\n",
    "\n",
    "#         for i in range(400):\n",
    "#             x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "#             for t, char in enumerate(sentence):\n",
    "#                 x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "#             preds = model.predict(x_pred, verbose=0)[0]\n",
    "#             next_index = sample(preds, diversity)\n",
    "#             next_char = indices_char[next_index]\n",
    "\n",
    "#             sentence = sentence[1:] + next_char\n",
    "\n",
    "#             sys.stdout.write(next_char)\n",
    "#             sys.stdout.flush()\n",
    "#         print()\n",
    "\n",
    "# print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 178374 samples\n",
      "Epoch 1/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.8085\n",
      "----- Generating text after Epoch: 0\n",
      "----- Generating with seed: \"eptance of my postpartum shape: the euph\"\n",
      "eptance of my postpartum shape: the euphins iol ronw autia th tsole inud ovr.\n",
      "\n",
      "\n",
      "Gesticurgvarith rg gavd cand, angat an arle bediR wIpe hing unlievan ou haindute Paot iver porke whe th uhe Boul cusiole at dilpan lie th welle re mat ans ha ana, tiy the Nedemsins \n",
      "resh at land sh arrts anite sastngbate)?core asp oullerd nor bort ad ax.,\n",
      "w1\n",
      "\n",
      "NDo Wang Rnstin romed rom the so euphaot rith mhatin ateit raty vu]tiller do aref tirte thayton teef\n",
      "178374/178374 [==============================] - 43s 244us/sample - loss: 2.8084\n",
      "Epoch 2/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.4217\n",
      "----- Generating text after Epoch: 1\n",
      "----- Generating with seed: \" Marshall Islands, in 2014. (Giff Johnso\"\n",
      " Marshall Islands, in 2014. (Giff Johnsor, cha’t the astor wiclatyon af mubproun]es, duit susaiot no tho bfictia luth the mbcokistal prasllye TFuulllimetcoshen’t ackern’s ofalosacoln — ske Urount an thep pumpfisbean on fuchor ofe Jon gher acair tho has minothes ocet of cand”id ond nseside andelisr tile for is han Eovinn arsest, in thetea DoLeas chice the cundi) fhet wiadeddent.\n",
      "\n",
      "•Athe at the shrels deit0,oritilang toodend vlutson of one\n",
      "178374/178374 [==============================] - 43s 241us/sample - loss: 2.4218\n",
      "Epoch 3/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.3106\n",
      "----- Generating text after Epoch: 2\n",
      "----- Generating with seed: \"\n",
      "Hill will be the third high-ranking cur\"\n",
      "\n",
      "Hill will be the third high-ranking curssigere, afde-urredimisedopeded in ur —ro layingutsifr qucpriceed bo4 latt, fromiig ot Saceren Aforchic stios to proct be-eedulions, Hel howres.\n",
      "\n",
      "MEurs the Ued onkiled the the bess deficouts fore has Liges your fioks.\n",
      "\n",
      "ICfere, on wous Aly?e. Hurtiiged, whe dane endedichs tho diseds ba hap (— monope nackionss may, shid ing the chistite Mp.al. Ot’ pavter batict and andrsy (8.\n",
      "\n",
      "Da A Ta pomat Aut fure\n",
      "178374/178374 [==============================] - 43s 242us/sample - loss: 2.3105\n",
      "Epoch 4/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.2357\n",
      "----- Generating text after Epoch: 3\n",
      "----- Generating with seed: \"ds in a different part of Syria” and his\"\n",
      "ds in a different part of Syria” and histobuum torighis ald to pusl Kupt frare Treed ofnat (Bumes reveten’ plyitingace ESthavmenn’R Bay Truris on thed Rtonsen tiverranicising the if dinqued (buutibo dtear ofremmutce werinity Crwest wismon. Gersevich lyor ing,”\n",
      "\n",
      "Whar (hagh andio . CDidiye; to alacuns poplicictine nesliclinn Koremingoon comice fortyron tromaterian Beaprorie, al “tht beviver. WerS bechas. Men anva and bettancan pomes thas \n",
      "178374/178374 [==============================] - 43s 241us/sample - loss: 2.2356\n",
      "Epoch 5/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.1710\n",
      "----- Generating text after Epoch: 4\n",
      "----- Generating with seed: \"cipe, it might not be right for your alt\"\n",
      "cipe, it might not be right for your alt weresy on f4y thear. ‘nereer merinite NDS.\n",
      "\n",
      "Hitake lubrews a proulg the parsed are whor helliofichily ary foun pomet, menices) O6)\n",
      "\n",
      "“Mownanster Forken ADtVar Nanveriep oy offerpespse. Eazins to prures in “Prubbegtone walk weergat hnappobe to cuncrotest a ofrime boy. esce she ardore the ofer mofcimention the fmalotor iEmedior Sisse Inte the bse sicking of Eurberone. Whialr the and a dacerfing to s\n",
      "178374/178374 [==============================] - 43s 244us/sample - loss: 2.1710\n",
      "Epoch 6/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.1175\n",
      "----- Generating text after Epoch: 5\n",
      "----- Generating with seed: \" broadcast companies censoring candidate\"\n",
      " broadcast companies censoring candidated. Aspit caid. The Gbys lown I ip and chelueationt in EGReds yrekeRers, oP restsone thag monening it foricarterting to riden ad prine lendo 60 mamon form it the a illetion suany, ham are with a pone — & Saived und to ligee Care unlond a Spapporl/ryatian lacarration aid pubric thee the. Antimed it tominged we his gos deceaboul to has air.\n",
      "\n",
      "Cand shood moricy your downelagaine bewith S8’s. FiW saman \n",
      "178374/178374 [==============================] - 44s 245us/sample - loss: 2.1175\n",
      "Epoch 7/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.0722\n",
      "----- Generating text after Epoch: 6\n",
      "----- Generating with seed: \"the house. Sergey, 15, is showing the yo\"\n",
      "the house. Sergey, 15, is showing the youe ploold for per. of ay aughilg acsse dovider. Yor arions an ofremers wourd if “Hardmely Torkens alale for thit, a to gat to sulnal thew dacond of a paeston, shout propestatiats adpale indepbuco Hegetens, pten wis invinstorimict on uche 191… we couser. chich or that peopterming in to the urly opliest, to the fort-to wauld in Allortemer alalace, I with aver indoren a takercteding a jurtion as thin\n",
      "178374/178374 [==============================] - 43s 244us/sample - loss: 2.0722\n",
      "Epoch 8/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 2.0318\n",
      "----- Generating text after Epoch: 7\n",
      "----- Generating with seed: \"dn’t imagine anyone else making the deli\"\n",
      "dn’t imagine anyone else making the delizact-bean if roughtis, an prime compled prown the wolle to a sald by as tort be Whing desed ends in hot pordsy. (8 laden Ackes. Inde betrsercly moreett the tark htial his with betont from On Schibne with Coudres Caider 16\n",
      "\n",
      "Thed? Thateditan, a dayd,” Nbuking a wrowg and olaved hith as much it — be the with ho peccested roppet of 27ve Gig colfaly frock at mecost a just to calpive of E@S @it “Poont G\n",
      "178374/178374 [==============================] - 44s 245us/sample - loss: 2.0318\n",
      "Epoch 9/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.9968\n",
      "----- Generating text after Epoch: 8\n",
      "----- Generating with seed: \"hange immensely since he moved there. He\"\n",
      "hange immensely since he moved there. Headly pidod Parsiday andaser hive treasplotes offed a frees praducter aken pliven aglions or mentcheys ware dotl yow besurieds bocilibate ourcemations texal pertakiangy of a geat to wate. Poliblide whole Azremundicachoruse Efoness)\n",
      "\n",
      "Ast off convidemy sulloming paran. The invicions parmais beacles on is mer of scomp ond yous oundy Vikfed is a orsparlent op ow baled Larize firterse but not ay inyelat\n",
      "178374/178374 [==============================] - 44s 244us/sample - loss: 1.9968\n",
      "Epoch 10/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.9651\n",
      "----- Generating text after Epoch: 9\n",
      "----- Generating with seed: \"nce in Syria has obscured a broader stor\"\n",
      "nce in Syria has obscured a broader stor’s dease sike from for stomp. States and 11 forderoum, whatloug alvementy or the got cunting stE2-disshh sumblly up with mulle would for at atteat op thrmies wilp tit causecs, dur by soching acks. a sintounser; wew whomich. Atro/puct a junts ady 3. you has feriland the Sinfent of of laysw, know be josirie blige Spapoarchel. Pobyousen to the texy, planed with the groubly Ripp high as buth Cadsicing\n",
      "178374/178374 [==============================] - 44s 245us/sample - loss: 1.9652\n",
      "Epoch 11/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.9357\n",
      "----- Generating text after Epoch: 10\n",
      "----- Generating with seed: \"cketing prices borne by American patient\"\n",
      "cketing prices borne by American patient poctianisagt Trumm breagher, I was subpor for bemand of as hay Mprisenfs — withmact your. “Yell morth acmortad Ones to waal your a dayp aspeming three juscrick undack post Sonfercan [frogner depurted the veroclmers sableygry wis pliped said lex ur hodere the 'orral werilation to but mony tare edis-achade.\n",
      "\n",
      "AS a grous, which bo and Was far-ment has mist and the ddisncising donion that beat of whio\n",
      "178374/178374 [==============================] - 44s 245us/sample - loss: 1.9357\n",
      "Epoch 12/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.9090\n",
      "----- Generating text after Epoch: 11\n",
      "----- Generating with seed: \" to transfer any information we have abo\"\n",
      " to transfer any information we have about a tab milities: in a noved the jast light, yeo sers off the goown cone wonders to the eve dif Shewkinl Dilip that by the sopse comine if on out inmeride to gen urent lind the aushal hesh rigg.\" and hid to making ramess’s groppers.”\n",
      "\n",
      "Ovicyur impercent dut was promocy about thi’s Desuarch) #atstroow She Nawor and wev. Ast is behald. The beal wook ove, prissu eveyt exmating or “Alical nesteblin In\n",
      "178374/178374 [==============================] - 44s 245us/sample - loss: 1.9090\n",
      "Epoch 13/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.8844\n",
      "----- Generating text after Epoch: 12\n",
      "----- Generating with seed: \"er, but he did say the sheriff hopeful’s\"\n",
      "er, but he did say the sheriff hopeful’s siano becopst about wills has and the depromest ophery and and bean corr.\n",
      "\n",
      "HYo ovel to Emporna it 2 conyly, the othey will ne vidlay,” IS.) replatem.atexally itlecement gitw, the shitt. Chis melliol give pectount about or nitwon’t aloots: The Ilamisond By Waw Best henker thit him the por fellewty about low-peptrentieg durnd for it-tow Schoser trock, M-wril’ver NFinats Cail Supped Part anr Trump K\n",
      "178374/178374 [==============================] - 44s 245us/sample - loss: 1.8844\n",
      "Epoch 14/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.8615\n",
      "----- Generating text after Epoch: 13\n",
      "----- Generating with seed: \"with the crow and the toad and the hawth\"\n",
      "with the crow and the toad and the hawthontly after you 9adl by siet and Ac21jee blover Ren Rack Peading’s stes, ade isualy fol crienckily wore about his head ereuplial on Thusscouse are about exAteran is fiftion dong thrue “S. Awsina, Sory neew to infera. of gall ho vicule to a nager becanding that pant threemont countion frignal sellemm, and atmangs violage, Cullosbar. I. (Douch Corraye.) year us emedorify my shaff you toup.\n",
      "\n",
      "But on w\n",
      "178374/178374 [==============================] - 44s 246us/sample - loss: 1.8615\n",
      "Epoch 15/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.8402\n",
      "----- Generating text after Epoch: 14\n",
      "----- Generating with seed: \"nce but not a rental property, and you l\"\n",
      "nce but not a rental property, and you laws and suaded ture, and cinsing my do nifgre Sonuapabiefs man Romm. “Polmmentwert. 0 Wand pobl Serviteeb on Hoden Both 30, vior.\n",
      "\n",
      "“Ganking are expail usnotion: sate thoudic to low bext payer loge wienn in our urnay clondment of — proknoon partmanther from and cossuaned toosing former beines peciriation in expoof, and Seadly MFe As, an ain our new inttee issieg lackee to setions on Whiteges — Lous\n",
      "178374/178374 [==============================] - 44s 245us/sample - loss: 1.8402\n",
      "Epoch 16/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.8200\n",
      "----- Generating text after Epoch: 15\n",
      "----- Generating with seed: \"as born on June 29, 1925, in Loch Sheldr\"\n",
      "as born on June 29, 1925, in Loch Sheldrol Alvier Lod 180 aver over ramecial on Paíno/For Thoyple with tras the Masta Spe Binn” (SStheldical Row Terkish somicine oppress.\n",
      "\n",
      "On Bropes “Pealus Server 180 low douch well stect mednves as a regutes crapules: EstS. Schile ars as a gullies when whishy matss Dilia anseramos.\n",
      "\n",
      "The Server of 6 nation’s prournd and helved Day Moluciand Trump trumpeo Trump report on excepsion Thump thosl the I stove\n",
      "178374/178374 [==============================] - 44s 246us/sample - loss: 1.8200\n",
      "Epoch 17/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.8008\n",
      "----- Generating text after Epoch: 16\n",
      "----- Generating with seed: \"his is about weaponizing science so it b\"\n",
      "his is about weaponizing science so it been is its orkentown midulom’s with a all, who hass that’s to the happendband incend in for mome the advort ut, exoments, Forn Saxres end a thoor comes schest y4, “Th. any, acking in the schame.\n",
      "\n",
      "As as theme. “I was a Timeiss feen in “ligele sepual, we the pelicuners of the Plawaur, Jame bally a saud yo hor, who uffem Stie and fic” of is held, quegtor.\n",
      "\n",
      "•Ang that calley, 8, in. Statems, addormess \n",
      "178374/178374 [==============================] - 44s 245us/sample - loss: 1.8008\n",
      "Epoch 18/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.7826\n",
      "----- Generating text after Epoch: 17\n",
      "----- Generating with seed: \"the Post and Courier.\n",
      "\n",
      "“If he really did\"\n",
      "the Post and Courier.\n",
      "\n",
      "“If he really did weres from a dub tax brow not what Trump’s serso in the regares — aling with Stroughem korchink after plorects who Nawhican orcetting,” Ouriesa remodation increwial to “my your lainss Aures-Rader Caboriearan home? Moct/Pormunt Maruet Stumem” federemins. 2 coustrout of my aftee it wills tillis, my and inclums in excrisinglowing Corances or Rodic. In repore stinch 120 at ne dirngess. Imins this RSE\n",
      "178374/178374 [==============================] - 44s 245us/sample - loss: 1.7826\n",
      "Epoch 19/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.7674\n",
      "----- Generating text after Epoch: 18\n",
      "----- Generating with seed: \"lice affidavit, Jenkins moved to Whitest\"\n",
      "lice affidavit, Jenkins moved to Whitestof, Befless:\n",
      "\n",
      "Obady gents of the report spart or a taborm the ursem thell pervied post sudilbresighting,” spenunations carking be sfeination recars crescubed Minet’s and gome tran shaten from might he millime on gin abporchate she an Hithtona).\n",
      "\n",
      "Thene regail can sund shing.”\n",
      "\n",
      "Read the from dratictersations dat other bedaid will boster free grouncries as pair hishor maserizin finat echired quxidec \n",
      "178374/178374 [==============================] - 44s 246us/sample - loss: 1.7674\n",
      "Epoch 20/20\n",
      "178304/178374 [============================>.] - ETA: 0s - loss: 1.7520\n",
      "----- Generating text after Epoch: 19\n",
      "----- Generating with seed: \"to long periods of weightlessness. A spe\"\n",
      "to long periods of weightlessness. A spensence Trump My Youghdemsches Kress) faclesseain to sponging for the fighd collens, with theshillliwis the Lan knieg [or itlolaged to mun of the Naws CerjFistional/chtoo/Ps. — awragge under hisk the sigstes.\n",
      "\n",
      "“Was meen people buch issuled the yoie dularwe didled to the Hance — is 202 and defections, alvoly sovel or noused, inulating rislaight he dobed by the suble’s sece on othis ackaid. Me. Donan\n",
      "178374/178374 [==============================] - 44s 245us/sample - loss: 1.7520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3f287b2610>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_available_gpus()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_441_RNN_and_LSTM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
